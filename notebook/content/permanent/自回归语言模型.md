---
title: "自回归语言模型"
date: "2023-06-15"
tags:
- AI
---
自回归语言模型 Autoregressive LM

通常的语言模型是单向的, 比如从左到右去预测下一个单词. 因为有时候除了考虑上文的信息还要考虑下文, 所以又诞生了从右到左预测的反向语言模型. 自回归模型指的就是这种只能利用上文或下文, 不能同时利用两者的模型.

典型的自回归语言模型如使用 Transformer 的 GPT, ELMo 这种将上文和下文的 LSTM 拼接的也算变形的自回归语言模型.

对应: [[permanent/自编码语言模型|自编码语言模型]]

参考: [自回归语言模型 VS 自编码语言模型 - 知乎](https://zhuanlan.zhihu.com/p/163455527)

[预训练语言模型综述(中) -- GPT &amp; BERT - 知乎](https://zhuanlan.zhihu.com/p/441359003)
