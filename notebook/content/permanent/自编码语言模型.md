---
title: "自编码语言模型"
date: "2023-06-15"
tags:
- AI
---

自编码语言模型 Autoencoder LM

自编码语言模型在预训练时, 会随机 Mask 掉输入 X 的一些单词, 然后训练的任务就是根据上下文预测这些单词.
相比于单向的[[permanent/自回归语言模型|自回归语言模型]], 做阅读理解的能力会更好. 但因为在 Fine-Tuning 阶段是看不到 Mask 标记的, 可能会导致预训练和FT阶段的不一致.


参考: [自回归语言模型 VS 自编码语言模型 - 知乎](https://zhuanlan.zhihu.com/p/163455527)

[预训练语言模型综述(中) -- GPT &amp; BERT - 知乎](https://zhuanlan.zhihu.com/p/441359003)